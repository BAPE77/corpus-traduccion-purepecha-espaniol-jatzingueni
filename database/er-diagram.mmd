%% ============================================================================
%% J'atzingueni Corpus Database - Entity Relationship Diagram (Mermaid)
%% ============================================================================
%% This diagram represents the complete database schema for the
%% Pur√©pecha-Spanish parallel corpus with support for:
%% - Morphological annotations (agglutinative features)
%% - Automated and manual workflows
%% - Quality metrics tracking
%% - Dialectal variation (PostGIS)
%% ============================================================================

erDiagram
    %% ========================================================================
    %% CORE ENTITIES: Sources and Documents
    %% ========================================================================
    
    source ||--o{ document : "contains"
    source {
        uuid id PK
        varchar name
        enum type "jw_org, biblical, manual, community"
        text url
        text description
        %% TODO: metadata should be its own defined columns
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    document {
        uuid id PK
        uuid source_id FK
        %% TODO: Should we remove this and just use title?
        varchar identifier "e.g., Genesis_1"
        varchar title
        enum language "tsz, es, en"
        %% TODO: expand metadata in its own columns
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    %% ========================================================================
    %% SENTENCES: Core linguistic data
    %% ========================================================================
    
    document ||--o{ sentence : "contains"
    sentence {
        uuid id PK
        uuid document_id FK
        integer index
        enum language "tsz, es, en"
        %% TODO: Consider a unique clean sentence as "text"
        %%       instead of original_text and normalized_text
        text text "Original sentence"
        text normalized_text "Cleaned version"

        tsvector text_vector "Full-text search"

        %% TODO: consider removing morphemes and just use morphosyntactic annotations
        %%       and some kind of index for spliting the word into morphemes
        text[] morphemes "Morpheme array"
        text[] morphological_tags "Tags for morphemes"
        %% TODO: remove tsz dialect 
        enum dialect "central, western, eastern"
        jsonb dialectal_features
        geometry collection_location "PostGIS point"
        jsonb metadata
        enum quality_status "raw, auto_aligned, reviewed"
        timestamp created_at
        timestamp updated_at
    }
    
    %% ========================================================================
    %% ALIGNMENTS: Sentence pair alignments
    %% ========================================================================
    
    sentence ||--o{ alignment : "tsz_sentence"
    sentence ||--o{ alignment : "es_sentence"
    alignment ||--o| alignment : "parent_correction"
    alignment {
        uuid id PK
        uuid tsz_sentence_id FK
        uuid es_sentence_id FK
        enum alignment_method "fast_align, awesome_align, manual"
        decimal alignment_score "0-1 confidence"
        enum quality_status "auto_aligned, reviewed, validated"
        jsonb word_alignments "Word-level data"
        varchar corrected_by "User who corrected"
        text correction_notes
        integer version "Version control"
        uuid parent_alignment_id FK "Previous version"
        timestamp created_at
        timestamp updated_at
    }
    
    %% ========================================================================
    %% MORPHOLOGICAL ANNOTATIONS: Token-level analysis
    %% ========================================================================

    sentence ||--o{ morphological_annotation : "has"
    %% TODO: Make morphological annotations just be the allowed symbols for annotations,
    %%       not the actual annotations themselves!
    morphological_annotation {
        uuid id PK
        uuid sentence_id FK
        integer token_index "Position in sentence"
        text token "Surface form"
        text[] morphemes "Decomposed morphemes"
        text[] morpheme_glosses "Glosses"
        text[] morpheme_types "root, affix, suffix"
        varchar pos_tag "Part of speech"
        varchar upos "Universal POS"
        jsonb features "Morphological features"
        varchar annotation_method "automatic, manual"
        varchar annotator
        decimal confidence_score
        timestamp created_at
        timestamp updated_at
    }
    %% TODO: include syntactic annotations too!

%% NOTE from Aaron-Uriel: I think we shouldn't focus on the next entities just
%%                        for now. These entities will be more useful if we
%%                        have a working developing plantform for real users.


    %% ========================================================================
    %% QUALITY METRICS: Pipeline and alignment quality
    %% ========================================================================
    
    pipeline_run ||--o{ alignment_quality_metric : "produces"
    alignment ||--o{ alignment_quality_metric : "measured_by"

    pipeline_run {
        uuid id PK
        varchar name
        varchar pipeline_type "collection, alignment, export"
        jsonb configuration "Pipeline parameters"
        timestamp started_at
        timestamp completed_at
        varchar status "running, completed, failed"
        text error_message
        integer items_processed
        integer items_succeeded
        integer items_failed
        jsonb metadata
    }
    
    alignment_quality_metric {
        uuid id PK
        uuid alignment_id FK
        uuid pipeline_run_id FK
        decimal alignment_accuracy "PRIMARY METRIC"
        decimal bleu_score "Secondary metric"
        decimal ter_score "Translation Error Rate"
        decimal length_ratio
        integer corpus_size_tokens
        integer vocabulary_size
        decimal oov_rate "Out-of-vocabulary"
        integer processing_time_ms
        jsonb metrics_data "Additional metrics"
        timestamp computed_at
    }
    
    corpus_statistic {
        uuid id PK
        timestamp computed_at
        integer total_sentence_pairs
        integer tsz_sentences
        integer es_sentences
        integer auto_aligned_count
        integer manually_reviewed_count
        integer validated_count
        decimal avg_alignment_accuracy
        decimal avg_alignment_score
        integer total_tsz_tokens
        integer total_es_tokens
        integer tsz_vocabulary_size
        integer es_vocabulary_size
        jsonb coverage_by_source "Distribution by source"
        jsonb dialect_distribution "Dialectal statistics"
        jsonb additional_stats
    }
    
    %% ========================================================================
    %% MANUAL ANNOTATION WORKFLOW: Human-in-the-loop
    %% ========================================================================
    
    annotation_task ||--o{ annotation_decision : "contains"
    
    annotation_task {
        uuid id PK
        varchar type "alignment_review, morphology"
        varchar assigned_to "User/annotator"
        integer priority "1-10 urgency"
        jsonb target_items "Items to review"
        varchar status "pending, in_progress, completed"
        integer progress "Percentage"
        timestamp created_at
        timestamp started_at
        timestamp completed_at
        text notes
        jsonb metadata
    }
    
    annotation_decision {
        uuid id PK
        uuid task_id FK
        varchar entity_type "sentence, alignment, morphology"
        uuid entity_id "Reference to entity"
        varchar decision_type "accept, reject, correct"
        jsonb previous_value
        jsonb new_value
        varchar annotator
        integer confidence "1-5 scale"
        text notes
        timestamp created_at
    }
    
    %% ========================================================================
    %% EXPORT: Data interoperability
    %% ========================================================================

    export_job {
        uuid id PK
        varchar format "tmx, conllu, json, pytorch"
        jsonb filter_criteria "What to export"
        text output_path
        bigint file_size_bytes
        varchar status "pending, running, completed"
        timestamp started_at
        timestamp completed_at
        integer records_exported
        jsonb metadata
    }
